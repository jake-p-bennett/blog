[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data science blog where I write about sports analytics, chess, and anything else that interests me."
  },
  {
    "objectID": "posts/time-in-chess/index.html",
    "href": "posts/time-in-chess/index.html",
    "title": "Time is Money — Even in Chess",
    "section": "",
    "text": "As an avid online chess player, it is extremely frustrating to have a winning position only to lose on time. Unfortunately, this happens to me quite frequently. When I watch the best players in the world, however, it seems that they can convert a winning position regardless of how little time is left on their clock (shout out to Eric Rosen). Generally speaking, it seems that higher rated players are better at managing their time, and are more adept at converting winning positions. I wanted to see if this was borne out in games played on Lichess. This gave me the idea to test out how time impacts the win probability of games across different skill levels.\nI built a logistic regression model that predicts outcomes using player ratings, material count, and time remaining. I analyzed bullet games where each player starts with 1 minute, and blitz games where players start with 3 or 5 minutes. I only considered games with no increment (i.e., only considered games where no time is added after each move)."
  },
  {
    "objectID": "posts/time-in-chess/index.html#the-data",
    "href": "posts/time-in-chess/index.html#the-data",
    "title": "Time is Money — Even in Chess",
    "section": "The Data",
    "text": "The Data\nWith the help of Claude, I wrote a python script that downloads games from Lichess’s public database. The script takes a player’s Lichess rating1 range as input, finds players who competed in recent Lichess tournaments within that rating range, and downloads a certain number of games from each of those players (I used a max of 500 games per player in my analysis). I downloaded games from three rating bands: 1000-1499, 1500-1999, and 2000-2499. Generally speaking, the 1000-1499 band contains novice to intermediate players, the 1500-1999 band contains intermediate to advanced players, and the 2000-2499 band contains advanced to master players.\nWith the games downloaded, I filtered out games with fewer than 10 moves, as it is unlikely that time played much of a role in these games. Then, I sampled a random position from each game, avoiding the first 5 moves and the last 5 moves. Sampling only one position from each games avoids the issue of data from the same game being heavily correlated. Not sampling positions from the first 5 moves reduces inclusion of positions that carry very little signal about the outcome of the game, while not sampling from the last 5 moves helps to reduce the amount of games that are nearly decided."
  },
  {
    "objectID": "posts/time-in-chess/index.html#the-model",
    "href": "posts/time-in-chess/index.html#the-model",
    "title": "Time is Money — Even in Chess",
    "section": "The Model",
    "text": "The Model\nFor each of these rating bands, I trained logistic regression models on 20,000 games each (80% training, 20% test), with a win for white as the response variable and the material difference between players, the rating difference between players, the move number, white’s time ratio (remaining time divided by started time), black’s time ratio, and the difference of these time ratios as the predictor variables. I trained nine models: one for each rating band and time control (bullet, 3-minute blitz, 5-minute blitz).\nIn blitz games, the rating difference and material difference are both more predictive than the time ratio difference. I think this is partially due to the fact that I downloaded games from Lichess tournaments. Pairing in tournaments works differently than normal pairing on Lichess in that players’ tournament standings are also taken into account. Because of this, a 2000-rated player joining late might face a 1400-rated player on a winning streak. These mismatches are probably inflating the importance of rating difference in the model.\nIn bullet chess, however, the time ratio difference matters substantially more. This is likely due to the fact that with 1 minute on the clock, every precious second matters. Spending even six seconds on a move removes 10% of your total time. Intuitively it makes sense that less starting time leads to time remaining being more important, which is what the data shows.\n\nLooking at the coefficients for the time ratio difference, we can see that my hunch about stronger players being better at converting winning positions under time pressure is not really true. While in bullet chess the time ratio matters less for players in the 2000-2499 range, the coefficient is pretty similar to those for the other rating bands. And in 5-minute blitz chess, the coefficient for the highest rating band is nearly identical to those of the lower rating bands. That is to say, all else equal, time pressure appears to affect all players in approximately the same way.\nAnother interesting finding from this analysis is that in 5+0 blitz, the coefficient for material balance decreases at higher ratings, but the opposite is true for bullet chess (in 3+0, it is around the same for all rating bands). This indicates that the material difference matters more for stronger players in faster games, but less in slower games. In bullet games, perhaps this speaks to strong players’ offensive abilities. Up on material, they can press their advantage relentlessly and even somewhat recklessly given the quick nature of the games. In blitz games, this might be a sign of strong players’ defense. With more time to spare, they may be able to find ways to hold on even when down a pawn or two."
  },
  {
    "objectID": "posts/time-in-chess/index.html#how-good-is-the-model",
    "href": "posts/time-in-chess/index.html#how-good-is-the-model",
    "title": "Time is Money — Even in Chess",
    "section": "How Good is the Model?",
    "text": "How Good is the Model?\nTo analyze the model itself, I performed a calibration analysis and an error analysis. For the calibration analysis, I compared predicted probabilities to actual win rates and tracked the distribution of predictions. Interestingly, the predictions for the 1000-1499 and 1500-1999 rating bands appear to be roughly normal, while the predictions for the 2000-2499 rating band have more predictions in the 0 to 0.1 and 0.9 to 1 range. This may be because stronger players are better at converting winning positions. In general, the model is well-calibrated across time controls and rating bands, with all Brier scores less than 0.2. In other words, when the model predicted a 70% chance of white winning, white actually won about 70% of the time (same for 80%, 90%, or any predicted probability).\nFor the error analysis, I inspected several games where the model was very confident but ended up being wrong. While the most confident predictions generally come from games where there is a large rating gap (because of the size of the coefficient), it doesn’t appear that there is one particular type of game or position that the model consistently gets very wrong. For example, in this 3-minute blitz game, for the sampled position, black is up a bishop, up on time, and rated 440 points higher than white. The model predicts a 3.8% chance of white winning.\n\nHowever, just a few moves later, black misses a fork that blunders the black queen, which white finds. Black resigned.\n\nIn another blitz game, white is up the exchange (i.e., has a rook in exchange for a knight) as well as two extra pawns, and is rated 399 points higher than black. On top of that, black has 11 seconds left on their clock. The model gives white a 95.6% chance of winning.\n\nBlack then proceeds to move extremely quickly, causing white to run out of time and lose the game.\n\nAs you might expect, many of the errors from the bullet games are due to the favored side running out of time. One egregious error came from this game where white is completely crushing black, as we might expect given the 674 point rating gap. The problem is, white has one second left. Despite this, the model gives white a 99.4% chance to win, with the rating difference and material advantage overpowering the time situation. Unsurprisingly, white lost on time."
  },
  {
    "objectID": "posts/time-in-chess/index.html#whats-next",
    "href": "posts/time-in-chess/index.html#whats-next",
    "title": "Time is Money — Even in Chess",
    "section": "What’s Next?",
    "text": "What’s Next?\nIt would be interesting to add Stockfish evaluation as a feature. At first glance we might think that engine evaluations would dominate the win probability, but given the inclusion of games with huge rating differences, I’m not so sure. One thing I did not consider for my analysis was correlation between features. Down the road this may be worth looking into, as multicollinearity may impact the interpretation of the coefficients.\nThe code for this analysis, including scripts for downloading games, extracting features, and training models, is available on GitHub.\nIn sum, my analysis found that time pressure affects all players pretty equally, especially in 5+0 blitz chess. While World Chess Champion Emanuel Lasker supposedly said, “When you see a good move, look for a better one,” in blitz chess, it’s probably worth it to just play the good move."
  },
  {
    "objectID": "posts/time-in-chess/index.html#footnotes",
    "href": "posts/time-in-chess/index.html#footnotes",
    "title": "Time is Money — Even in Chess",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLichess uses the Glicko-2 system, though the term ELO is still widely used to denote a player’s strength. This analysis uses Glicko-2 ratings.↩︎"
  },
  {
    "objectID": "posts/nfl-betting/index.html",
    "href": "posts/nfl-betting/index.html",
    "title": "Betting on The Athletic’s NFL Projections",
    "section": "",
    "text": "During the 2024-2025 NFL season, The Athletic maintained an NFL projections webpage that contained forecasts for each regular season and playoff game. The projections included forecasted points spreads, and I was curious to see how these forecasts compared to the Las Vegas spreads and the actual game outcomes. I back-tested a simple betting strategy:\n\nWhen The Athletic’s spread is greater than the Vegas spread, bet on the favorite.\nWhen The Athletic’s spread is less than the Vegas spread, bet on the underdog.\nIf the spreads are the same, do not place a bet.\n\nThe strategy only makes spread bets (i.e., no moneyline bets), and assumes -110 odds (wagering $110 to win $100).\nOf the 285 NFL games in the 2024-2025 season (regular season and playoffs), there were 239 games where The Athletic’s spread did not match the Vegas spread. On average, The Athletic’s spreads differed from the actual game outcome by 9.75 points, while Vegas’s spreads differed by 9.66 points. The median differences for The Athletic and Vegas were 7.5 points and 7 points, respectively. These are minimal differences, to be sure, but the Vegas lines were more accurate for the 2024-2025 season.\nUnfortunately, the betting strategy described above does not give us an edge. Assuming bets of $110, employing the strategy would have lost $1,360 for a return of -5.17%. In fact, using the complete opposite strategy would have lost $940 for a return of -3.58%, which is better than the -4.55% we would expect from winning half of our bets.\nThen I wondered if The Athletic’s spread forecasts were better for a particular “direction” of spread difference. That is to say, what if we bet only on games where The Athletic had the favorite favored by more? What if we bet only on games where The Athletic had the favorite favored by less?\nIt turns out that betting only on games where The Athletic had the favorite favored by more than Vegas would have been profitable to varying degrees, depending on what threshold of spread difference is used.\n\nThe same cannot be said for games where The Athletic had the favorite favored by less:\n\nThe Athletic is still publishing weekly spread forecasts for the current 2025-2026 season (e.g., here is the link for Week 5). I will be monitoring these projections to see if they fare better or worse than last year’s.\nIf you are curious to see more, you can download the analysis workbook and the data for this post."
  },
  {
    "objectID": "posts/nba-lineups/index.html",
    "href": "posts/nba-lineups/index.html",
    "title": "In Basketball, Attention Is Not All You Need",
    "section": "",
    "text": "The attention mechanism revolutionalized the fields of deep learning and natural language processing by introducing a way to model the interactions between words in a nuanced way. I was curious to see if this type of interaction modeling could translate to the chemistry between NBA players. Intuitively it feels like this might work. Nikola Jokic is clearly a dominant player on his own, but surrounding him with lethal shooters and skilled perimiter defenders seems like a better option than pairing him with a ball-dominant defensive liability (like Trae Young). I thought it might also be possible to model lineup matchups in this way, with certain defensive lineups able to stymie certain types of offenses more easily.\nIn searching for prior work on this topic, I found the NBA2Vec paper, which showed that player embeddings could capture positional and stylistic similarities. Their approach averaged embeddings across lineups (which I will refer to as the baseline model), losing pairwise interaction information.\nI hypothesized that attention mechanisms could do better by: - Using self-attention to model within-team synergies (e.g., pick-and-roll partners) - Using cross-attention to model offensive vs. defensive matchups"
  },
  {
    "objectID": "posts/nba-lineups/index.html#the-data",
    "href": "posts/nba-lineups/index.html#the-data",
    "title": "In Basketball, Attention Is Not All You Need",
    "section": "The Data",
    "text": "The Data\nI used a dataset of play-by-play NBA data from Kaggle that contains data from the 2015-2022 seasons (only partial data from the 2022 season). The data contained around 4.4 million plays across 9,456 games. To eliminate some noise, I only looked at players with over 100 possessions played, of whom there were 1,181. I wanted the model to predict point differential for the home team."
  },
  {
    "objectID": "posts/nba-lineups/index.html#the-models",
    "href": "posts/nba-lineups/index.html#the-models",
    "title": "In Basketball, Attention Is Not All You Need",
    "section": "The Models",
    "text": "The Models\nTo start, I mimicked the NBA2Vec methodology1, creating a model that learns player embeddings from scratch to predict point differential. Then, I created an attention-based model that uses self-attention for players on the same team, cross-attention for players on different teams, and then sends a possession-weighted average of lineups for each team through a multilayer perceptron to come up with a prediction. Both of these models had an R-squared of around 0, showing that the neural net cannot learn from these random embeddings without some kind of pretraining. To address this issue, I used vectors containing player stats (offensive rating, defensive rating, net rating, possessions, and games played) as embeddings, and tested my original approach both at the player level and at the team level. Attention did not help in this case either, actually making the R-squared worse in each case.\nI then decided to try a hybrid approach, using player stats as embeddings but having the model learn residuals. Unfortunately, this method also failed, giving essentially the same R-squared values for both the baseline model and the attention model. Even when adding two more features, player points per 100 possessions2 and rebounds per 100 possessions, the attention model failed to outperform the baseline model, with both R-squared values refusing to go any higher than 0.2\n\n\n\nModel\nParameters\nTest R²\nTest RMSE\nWin Accuracy\n\n\n\n\nEmbeddingsBaseline\n83K\n0.001\n26.7\n54.9%\n\n\nEmbeddingsAttention\n176K\n0.000\n26.7\n56.0%\n\n\nStatsBaseline (team-level)\n5K\n0.202\n12.7\n66.5%\n\n\nStatsAttention (team-level)\n42K\n0.193\n12.7\n66.2%\n\n\nStatsPlayerBaseline\n9K\n0.201\n12.7\n65.0%\n\n\nStatsPlayerAttention\n17K\n0.191\n12.8\n66.1%\n\n\nHybridBaseline\n46K\n0.203\n12.7\n65.8%\n\n\nHybridAttention\n55K\n0.199\n12.7\n65.6%\n\n\nHybridBaseline (rich)\n46K\n0.203\n12.7\n66.2%\n\n\nHybridAttention (rich)\n55K\n0.200\n12.7\n66.7%\n\n\n\nAs a sanity check, I compared the models’ results against XGBoost with different feature representations. I used three different feature configurations:\n\nAveraged: Weighted mean of player stats per team (14 features)\nSummary: Mean, std, min, max of player stats per team (56 features)\nConcatenated: Top-10 players’ stats concatenated (160 features)\n\nWhile the first two configurations performed around the same as the other models that I tried (R-squared around 0.2), the concatenation method gave an R-squared of 0.29. This makes some sense. Running XGBoost on these concatenated vectors allows the model to deal with non-linear relationships, which the averaging-based neural net can’t do, without having to learn tens of thousands of parameters, which the attention-based neural net has to do.\n\n\n\nModel\nFeatures\nTest R²\nWin Accuracy\n\n\n\n\nXGBoost (averaged)\n14\n0.195\n65.3%\n\n\nXGBoost (summary)\n56\n0.196\n65.2%\n\n\nXGBoost (concat)\n160\n0.289\n65.2%\n\n\n\nI didn’t want to give up just yet, and I hypothesized that the game-level averaging by minutes played for each lineup could be obscuring lineup specific effects. So I ran a separate experiment at the lineup level using the same hybrid approach described above. Unfortunately this also didn’t work, with the models having basically no predictive power whatsoever. This is probably because this lineup approach does not account for strength of opponents at all.\n\n\n\nModel\nTest R²\n\n\n\n\nLineupBaseline\n0.0225\n\n\nLineupAttention\n0.0200"
  },
  {
    "objectID": "posts/nba-lineups/index.html#interpretation",
    "href": "posts/nba-lineups/index.html#interpretation",
    "title": "In Basketball, Attention Is Not All You Need",
    "section": "Interpretation",
    "text": "Interpretation\nIt’s hard to conclude that lineup chemistry doesn’t exist in the NBA. But this approach didn’t detect it. It’s possible that the efficiency stats I used don’t capture the right signal or that the sample size was too small. It’s also possible that some other kind of attention mechanism would perform better. It could be that synergy is real but too small to be detected when drowned out by individual player quality and noise. I think it’s probably some combination of all of these.\nTaking a step back, all of the models I tested had a win accuracy of around 65-67% (except for the models that had to learn embeddings from scratch). This means that while the point differential predictions weren’t the most accurate, they were directionally pretty sound. Indeed, as of January 13, 2026, NBA favorites have a win rate of 64.5%, indicating that the models do have some predictive value. Without considering injuries, rest days, or home court advantage, the models I tested basically matched (they slightly exceeded, but it’s hard to say that’s not just noise) the Vegas oddsmakers’ predictions of who will win games, which I think is a pretty good result.\nThe code for this project, including scripts for downloading play-by-play data, extracting stats, and training models, is available on GitHub.\nWhile I couldn’t get the attention mechanism to model NBA player interactions, it was still fun to play around with these models and learn something new. In the future I’d like to do more work on statistical learning related to NBA stats."
  },
  {
    "objectID": "posts/nba-lineups/index.html#footnotes",
    "href": "posts/nba-lineups/index.html#footnotes",
    "title": "In Basketball, Attention Is Not All You Need",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough they predicted play outcomes, e.g., made two-point basket, missed three-point attempt, etc.↩︎\nNote that this differs from offensive rating which is the player’s team’s points per 100 possessions when they are on the court.↩︎"
  },
  {
    "objectID": "posts/nfl-betting/notebooks/nfl-notebook.html",
    "href": "posts/nfl-betting/notebooks/nfl-notebook.html",
    "title": "Jake's Blog",
    "section": "",
    "text": "from bs4 import BeautifulSoup\nimport pandas as pd\nimport numpy as np\nimport glob\nimport os\n\n\n# Setting up some dictionaries etc.\nteam_name_to_id = {'Cardinals': 'ARI', 'Falcons': 'ATL', 'Colts': 'IND', 'Ravens': 'BAL', 'Patriots': 'NE', 'Bills': 'BUF', 'Panthers': 'CAR', 'Bears': 'CHI', 'Bengals': 'CIN', 'Browns': 'CLE', 'Cowboys': 'DAL', 'Broncos': 'DEN', 'Lions': 'DET', 'Packers': 'GB', 'Oilers': 'TEN', 'Texans': 'HOU', 'Jaguars': 'JAX', 'Chiefs': 'KC', 'Raiders': 'LVR', 'Chargers': 'LAC', 'Rams': 'LAR', 'Dolphins': 'MIA', 'Vikings': 'MIN', 'Saints': 'NO', 'Giants': 'NYG', 'Jets': 'NYJ', 'Eagles': 'PHI', 'Steelers': 'PIT', '49ers': 'SF', 'Seahawks': 'SEA', 'Buccaneers': 'TB', 'Titans': 'TEN', 'Commanders': 'WAS'}\nlong_team_name_to_id = {'Arizona Cardinals': 'ARI', 'Atlanta Falcons': 'ATL', 'Baltimore Colts': 'IND', 'Baltimore Ravens': 'BAL', 'Boston Patriots': 'NE', 'Buffalo Bills': 'BUF', 'Carolina Panthers': 'CAR', 'Chicago Bears': 'CHI', 'Cincinnati Bengals': 'CIN', 'Cleveland Browns': 'CLE', 'Dallas Cowboys': 'DAL', 'Denver Broncos': 'DEN', 'Detroit Lions': 'DET', 'Green Bay Packers': 'GB', 'Houston Oilers': 'TEN', 'Houston Texans': 'HOU', 'Indianapolis Colts': 'IND', 'Jacksonville Jaguars': 'JAX', 'Kansas City Chiefs': 'KC', 'Las Vegas Raiders': 'LVR', 'Los Angeles Chargers': 'LAC', 'Los Angeles Raiders': 'LVR', 'Los Angeles Rams': 'LAR', 'Miami Dolphins': 'MIA', 'Minnesota Vikings': 'MIN', 'New England Patriots': 'NE', 'New Orleans Saints': 'NO', 'New York Giants': 'NYG', 'New York Jets': 'NYJ', 'Oakland Raiders': 'LVR', 'Philadelphia Eagles': 'PHI', 'Phoenix Cardinals': 'ARI', 'Pittsburgh Steelers': 'PIT', 'San Diego Chargers': 'LAC', 'San Francisco 49ers': 'SF', 'Seattle Seahawks': 'SEA', 'St. Louis Cardinals': 'ARI', 'St. Louis Rams': 'LAR', 'Tampa Bay Buccaneers': 'TB', 'Tennessee Oilers': 'TEN', 'Tennessee Titans': 'TEN', 'Washington Commanders': 'WAS', 'Washington Football Team': 'WAS', 'Washington Redskins': 'WAS'}\nlist_of_teams = ['Bills', 'Dolphins', 'Patriots', 'Jets',\n'Bengals', 'Steelers', 'Ravens', 'Browns',\n'Texans', 'Jaguars', 'Titans', 'Colts',\n'Chiefs', 'Chargers', 'Raiders', 'Broncos',\n'Cowboys', 'Eagles', 'Commanders', 'Giants',\n'Packers', 'Bears', 'Vikings', 'Lions',\n'Saints', 'Panthers', 'Falcons', 'Buccaneers',\n'49ers', 'Rams', 'Cardinals', 'Seahawks']\n\n\n# Setting up some functions\n\n# Gather all win probs for a certain week\ndef win_prob_weekX(weekX_data):\n    win_prob_dict = dict()\n\n    for team in list_of_teams:\n        win_prob = find_win_prob(team, weekX_data)\n        win_prob_dict[team] = win_prob\n\n    return win_prob_dict\n\n# Function that returns the win probability for a certain team\ndef find_win_prob(team, weekX_data):\n    text = weekX_data\n    \n    # Find index of team name\n    start_idx = text.find(team)\n\n    # Find the '%' after that\n    percent_idx = text.find('%', start_idx)\n\n    # Get the substring directly before '%'\n    win_prob = text[percent_idx-2:percent_idx]    \n    return win_prob\n\n# Function that returns the line for a particular team\ndef find_line(team, weekX_data):\n    text = weekX_data\n\n    # Find index of team name\n    start_idx = text.find(team)\n\n    # Find the spread signifier after that\n    spread_idx = text.find('&lt;div class=\"spread-stat svelte-1yqqaah\"&gt;', start_idx)\n\n    spread_html = text[spread_idx:spread_idx+100]\n    soup = BeautifulSoup(spread_html, \"html.parser\")\n\n    value = soup.find(\"div\", class_=\"spread-stat\").text\n    return value\n    \ndef lines_weekX(weekX_data):\n    line_dict = dict()\n\n    for team in list_of_teams:\n        try:\n            line = find_line(team, weekX_data)\n        except Exception:\n            continue\n\n        if line is not None:\n            line_dict[team_name_to_id[team]] = line\n    \n    return line_dict\n\n\n# Read in html data from The Athletic's Website\nall_weeks = {}\n\n# Base path to data folder\ndata_path = os.path.join(\"..\", \"data\")\n\nathletic_path = os.path.join(data_path, \"The Athletic\")\n# Regular season data\nfor filepath in glob.glob(os.path.join(athletic_path, 'week*.html')):\n    week_name = os.path.basename(filepath).replace('.html', '')\n    with open(filepath, 'r', encoding='utf-8') as f:\n        all_weeks[week_name] = f.read()\n\n# Playoff data\nfor playoff_file in ['Wildcard.html', 'Division.html', 'Conference.html', 'Superbowl.html']:\n    key = playoff_file.replace('.html', '')  # 'Wildcard', etc.\n    path = os.path.join(athletic_path, playoff_file)\n    with open(path, 'r', encoding='utf-8') as f:\n        all_weeks[key] = f.read()\n\n\n# --- Create dictionary of weekly line data ---\nall_weeks_line_dict = {week: lines_weekX(html) for week, html in all_weeks.items()}\n\n# --- Read in Spreadspoke CSV data ---\ndf_interim = pd.read_csv(os.path.join(data_path, 'Spreadspoke', 'spreadspoke_scores.csv'))\ndf = df_interim[df_interim['schedule_season'] == 2024].copy()\n\n# --- Create new columns for home and away team IDs ---\ndf['team_home_id'] = df['team_home'].map(long_team_name_to_id)\ndf['team_away_id'] = df['team_away'].map(long_team_name_to_id)\n\ndf['team_underdog_id'] = np.where(\n    df['team_favorite_id'] == df['team_home_id'],\n    df['team_away_id'],\n    df['team_home_id']\n)\n\n# Fix typo in original data\ndf.at[13833,'team_favorite_id'] = 'NYJ'\n\n# --- Create column for lines from The Athletic ---\ndf['calculated_line'] = None\n\nfor week_name, week_data in all_weeks_line_dict.items():\n    # For regular weeks, extract the week number (playoffs stay as is)\n    if week_name.lower().startswith('week'):\n        week_number = week_name.replace(\"week\", \"\")\n    else:\n        week_number = week_name  # 'Wildcard', 'Division', etc.\n\n    df.loc[df['schedule_week'] == week_number, 'calculated_line'] = (\n        df.loc[df['schedule_week'] == week_number, 'team_favorite_id'].map(week_data)\n    )\n\n\n# Fill in calculated_line. The blanks currently represent games where the Vegas favorite differs from The Athletic favorite\ndf['calculated_line'] = df['calculated_line'].replace(\"\",np.nan)\n\nfor week_name, week_data in all_weeks_line_dict.items():\n    week_number = week_name.replace('week', '')\n\n    mask = (df['schedule_week'] == week_number) & (df['calculated_line'].isna())\n\n    df.loc[mask, 'calculated_line'] = (\n        df.loc[mask, 'team_underdog_id']\n        .map(week_data)\n        .str[1:]\n    )\n\n\n# Convert pick-ems to 0\ndf.loc[df['calculated_line'] == 'PK', 'calculated_line'] = 0\ndf.loc[df['calculated_line'] == 'K', 'calculated_line'] = 0\n\n\n# Convert columns to numeric and take the difference\ndf['spread_favorite'] = df['spread_favorite'].astype(\"Float32\")\ndf['calculated_line'] = df['calculated_line'].astype(\"Float32\")\n\n# Calculate difference between calculated line and spread\ndf['diff'] = df['spread_favorite'] - df['calculated_line']\n\n# See if we should take action or not\ndf['action'] = None\n\n# If diff is negative, we're betting on the underdog\n# If diff is positive, we're betting on the favorite\ndf.loc[df['diff'] &lt; 0, 'action'] = 'bet on underdog'\ndf.loc[df['diff'] &gt; 0, 'action'] = 'bet on favorite'\n\n# Let's check to see how many games there are where The Athletic's spread does not match the Vegas spread\nn = df[abs(df['diff']) &gt; 0].shape[0]\nprint(f\"Number of games with mismatched spread: {n}\")\n\nNumber of games with mismatched spread: 239\n\n\n\n\n# Step 1: Determine if favorite is home or away\ndf[\"favorite_score\"] = np.where(df[\"team_favorite_id\"] == df[\"team_home_id\"],\n                                df[\"score_home\"],\n                                df[\"score_away\"])\n\ndf[\"underdog_score\"] = np.where(df[\"team_underdog_id\"] == df[\"team_home_id\"],\n                                df[\"score_home\"],\n                                df[\"score_away\"])\n\n# Step 2: Compute margin\ndf[\"margin_fav_underdog\"] = df[\"underdog_score\"] - df[\"favorite_score\"]\n\n\ndf['result_column'] = None\ndf['modeled_v_actual_vegas'] = abs(df['spread_favorite'] - df['margin_fav_underdog'])\ndf['modeled_v_actual_nyt'] = abs(df['calculated_line'] - df['margin_fav_underdog'])\ndf.at[14085, 'modeled_v_actual_nyt'] = 0\n\n\n\n\n# Check out in total how far off the Vegas lines and The Athletic's lines were\navg_residual_athletic = df['modeled_v_actual_nyt'].mean()\navg_residual_vegas = df['modeled_v_actual_vegas'].mean()\n\nmedian_residual_athletic = df['modeled_v_actual_nyt'].median()\nmedian_residual_nyt = df['modeled_v_actual_vegas'].median()\n\nprint(\"*** Averages ***\")\nprint(f\"The Athletic: {avg_residual_athletic: .2f}\")\nprint(f\"Vegas: {avg_residual_vegas: .2f}\")\nprint(\"\\n*** Medians ***\")\nprint(f\"The Athletic: {median_residual_athletic: .2f}\")\nprint(f\"Vegas: {median_residual_nyt: .2f}\")\n\n*** Averages ***\nThe Athletic:  9.75\nVegas:  9.66\n\n*** Medians ***\nThe Athletic:  7.50\nVegas:  7.00\n\n\n\n# Main function that calculates wins and losses. I added the \"fade\" argument after discovering that betting based on my initial idea was not a good strategy\n# My intial idea was to bet on the underdog in cases where The Athletic projected a smaller line than Vegas and to bet on the favorite in cases where The Athletic projected a bigger line than Vegas\n# Flipping the \"fade\" argument to true flips those actions\n\ndef bet_results(df: pd.DataFrame, fade: bool=False, threshold: float=0.0, mode: str=\"both\") -&gt; pd.DataFrame:\n    \n    df = df.copy()\n    df['result_column'] = None\n    diff = df['diff']\n\n    # Step 1: Fade picks if requested\n    if fade:\n        df.loc[df['action'] == 'bet on underdog', 'action'] = 'bet on favorite_'\n        df.loc[df['action'] == 'bet on favorite', 'action'] = 'bet on underdog_'\n\n    # Step 2: Threshold mask\n    if mode == \"positive\":\n        threshold_mask = diff &gt;= threshold\n    elif mode == \"negative\":\n        threshold_mask = diff &lt;= -threshold\n    elif mode == \"both\":\n        threshold_mask = (diff &gt;= threshold) | (diff &lt;= -threshold)\n    else:\n        raise ValueError(\"mode must be 'positive', 'negative', or 'both'\")\n\n    # Apply results only where threshold condition is satisfied\n    # Underdog bets\n    for und_key in ['bet on underdog', 'bet on underdog_']:\n        mask = (df['action'] == und_key) & threshold_mask\n        df.loc[mask & (df['margin_fav_underdog'] &gt; df['spread_favorite']), 'result_column'] = 100\n        df.loc[mask & (df['margin_fav_underdog'] &lt; df['spread_favorite']), 'result_column'] = -110\n        df.loc[mask & (df['margin_fav_underdog'] == df['spread_favorite']), 'result_column'] = 0\n\n    # Favorite bets\n    for fav_key in ['bet on favorite', 'bet on favorite_']:\n        mask = (df['action'] == fav_key) & threshold_mask\n        df.loc[mask & (df['margin_fav_underdog'] &lt; df['spread_favorite']), 'result_column'] = 100\n        df.loc[mask & (df['margin_fav_underdog'] &gt; df['spread_favorite']), 'result_column'] = -110\n        df.loc[mask & (df['margin_fav_underdog'] == df['spread_favorite']), 'result_column'] = 0\n\n    return df\n\n\n\n# Function to calculate and print ROI of a particular strategy\ndef print_roi(df: pd.DataFrame, fade: bool=False, threshold: float = 0.0, mode: str=\"both\"):\n    \n    results = bet_results(df=df, fade=fade, threshold=threshold, mode=mode)\n    win_loss = results['result_column'].sum()\n\n    if mode == \"both\":\n        investment = sum(results['result_column'].notna()) * 110\n    elif mode == \"positive\":\n        temp_df = results[results['diff'] &gt;= threshold]\n        investment = sum(temp_df['result_column'].notna()) * 110\n        win_loss = temp_df['result_column'].sum()\n    else:\n        temp_df = results[results['diff'] &lt;= threshold]\n        investment = sum(temp_df['result_column'].notna()) * 110\n        win_loss = temp_df['result_column'].sum()\n    roi = win_loss / investment\n\n    print(f\"Number of bets placed: {investment/110:,.0f}\")\n    print(f\"Total investment: {investment}\")\n    print(f\"Total winnings/loss: {win_loss}\")\n    print(f\"ROI: {roi:.2%}\")\n\n\n# How well would the initial betting strategy work?\n\ninitial = print_roi(df, fade=False, threshold=0, mode=\"both\")\n\n# Double check this works\ntest = bet_results(df=df, fade=False, threshold=0, mode=\"both\")\ntest_sum = test['result_column'].sum()\nprint(f\"\\nTest sum is {test_sum}\")\n\nNumber of bets placed: 239\nTotal investment: 26290\nTotal winnings/loss: -1360\nROI: -5.17%\n\nTest sum is -1360\n\n\n\n# How about employing the complete opposite strategy?\n\nopposite = print_roi(df, fade=True, threshold=0, mode=\"both\")\n\nNumber of bets placed: 239\nTotal investment: 26290\nTotal winnings/loss: -940\nROI: -3.58%\n\n\n\n# Gut-check to see what our ROI would be if we won 50% of our bets\nf_f = 100 + -110\ninvestment = 110*2\nroi = f_f / investment\nprint(f\"ROI when winning 50% of bets: {roi:.2%}\")\n\nROI when winning 50% of bets: -4.55%\n\n\n\n# What about games where The Athletic has the favorite favored by more (2 points more, in this case)?\ntest = print_roi(df=df, fade=False, threshold=2, mode=\"positive\")\n\nNumber of bets placed: 18\nTotal investment: 1980\nTotal winnings/loss: 540\nROI: 27.27%\n\n\n\n# Creeate a table for games where The Athletic has the favorite favored by more\nrows = []\nfor diff in [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]:\n    temp_df = bet_results(df=df, fade=False, threshold=diff, mode=\"positive\")\n    profit = temp_df[temp_df['diff'] &gt;= diff]['result_column'].sum()\n    num_games = (temp_df['diff'] &gt;= diff).sum()\n    investment = num_games * 110\n    roi = (profit / investment) * 100 if investment !=0 else None\n    \n    rows.append({\n    \"Spread Difference\": diff,\n    \"Number of Games\": num_games,\n    \"Profit\": profit,\n    \"Investment\": investment,\n    \"Return\": roi\n    })\n\ntable = pd.DataFrame(rows)\n\nstyled_table = (\n    table.style\n    .format({\"Return\": \"{:.2f}%\",\n             \"Investment\": \"${:,.0f}\",\n             \"Spread Difference\": \"\\u2265{:.1f}\",\n             \"Profit\": lambda x: f\"-${abs(x):,.0f}\" if x &lt; 0 else f\"${x:,.0f}\"\n    })\n    .set_table_styles([{\"selector\": \"th\", \"props\": [(\"background-color\", \"#f2f2f2\"), (\"font-weight\", \"bold\")]}])\n    .hide(axis=\"index\")\n)\n\nimport dataframe_image as dfi\ndfi.export(\n    styled_table,\n    \"../return_table.png\",\n    table_conversion=\"chrome\"\n)\n\n\n# Create a table for games where The Athletic's spread had the favorite favored by less\nrows = []\nfor diff in [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]:\n    diff = diff * -1\n    temp_df = bet_results(df=df, fade=False, threshold=diff, mode=\"negative\")\n    profit = temp_df[temp_df['diff'] &lt;= diff]['result_column'].sum()\n    num_games = (temp_df['diff'] &lt;= diff).sum()\n    investment = num_games * 110\n    roi = (profit / investment) * 100 if investment !=0 else None\n    \n    rows.append({\n    \"Spread Difference\": diff,\n    \"Number of Games\": num_games,\n    \"Profit\": profit,\n    \"Investment\": investment,\n    \"Return\": roi\n    })\n\ntable = pd.DataFrame(rows)\n\nstyled_table = (\n    table.style\n    .format({\"Return\": \"{:.2f}%\",\n             \"Investment\": \"${:,.0f}\",\n             \"Spread Difference\": \"\\u2264{:.1f}\",\n             \"Profit\": lambda x: f\"-${abs(x):,.0f}\" if x &lt; 0 else f\"${x:,.0f}\"\n    })\n    .set_table_styles([{\"selector\": \"th\", \"props\": [(\"background-color\", \"#f2f2f2\"), (\"font-weight\", \"bold\")]}])\n    .hide(axis=\"index\")\n)\n\nimport dataframe_image as dfi\ndfi.export(\n    styled_table,\n    \"../return_table2.png\",\n    table_conversion=\"chrome\"\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jake’s Blog",
    "section": "",
    "text": "In Basketball, Attention Is Not All You Need\n\n\nAttempting to predict NBA outcomes using attention mechanisms\n\n\n\nbasketball\n\nsports\n\n\n\n\n\n\nJan 13, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nTime is Money — Even in Chess\n\n\nHow does time impact the win probability of blitz chess games across different skill levels?\n\n\n\nchess\n\n\n\n\n\n\nJan 5, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nBetting on The Athletic’s NFL Projections\n\n\nDid The Athletic have an edge on Vegas during the 2024 NFL season?\n\n\n\nfootball\n\nsports\n\n\n\n\n\n\nOct 4, 2025\n\n\n\n\n\nNo matching items"
  }
]